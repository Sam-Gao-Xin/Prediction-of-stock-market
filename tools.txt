# -*- coding: utf-8 -*-

from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
import pandas as pd
import numpy as np
from sklearn.model_selection import GridSearchCV


def proc_text(raw_line):
    
    raw_line = str(raw_line)
    
    raw_line = raw_line.lower()

    # remove b'...' or b"..."
    if raw_line[:2] == 'b\'' or raw_line[:2] == 'b"':
        raw_line = raw_line[2:-1]

    tokenizer = RegexpTokenizer(r'\w+')
    tokens = tokenizer.tokenize(raw_line)
    meaninful_words = [w for w in tokens if w not in stopwords.words('english')]
    return ' '.join(meaninful_words)


def clean_text(raw_text_df):
    """
        clean_raw_text
    """
    cln_text_df = pd.DataFrame()
    cln_text_df['date'] = raw_text_df['Date'].values
    cln_text_df['label'] = raw_text_df['Label'].values
    cln_text_df['text'] = ''

    # process 25 rows od dataï¼Œ['Top1', ..., 'Top25']
    col_list = ['Top' + str(i) for i in range(1, 26)]

    for i, col in enumerate(col_list):
        raw_text_df[col] = raw_text_df[col].apply(proc_text)
        # combine column
        cln_text_df['text'] = cln_text_df['text'].str.cat(raw_text_df[col], sep=' ')
        print('has processed{}column.'.format(i + 1))

    return cln_text_df


def split_train_test(data_df):
    
    # time frame 2008-08-08 ~ 2014-12-31
    train_text_df = data_df.loc['20080808':'20141231', :]
    
    train_text_df.reset_index(drop=True, inplace=True)

    #  time frame 2015-01-02 ~ 2016-07-01
    test_text_df = data_df.loc['20150102':'20160701', :]
    
    test_text_df.reset_index(drop=True, inplace=True)

    return train_text_df, test_text_df


def get_word_list_from_data(text_df):
    
    word_list = []
    for _, r_data in text_df.iterrows():
        word_list += r_data['text'].split(' ')
    return word_list


def extract_feat_from_data(text_df, text_collection, common_words_freqs):

    n_sample = text_df.shape[0]
    n_feat = len(common_words_freqs)
    common_words = [word for word, _ in common_words_freqs]

    # initiation
    X = np.zeros([n_sample, n_feat])
    y = np.zeros(n_sample)

    print('extract features...')
    for i, r_data in text_df.iterrows():
        if (i + 1) % 100 == 0:
            print('has accomplished{}number of data sets'.format(i + 1))

        text = r_data['text']

        feat_vec = []
        for word in common_words:
            if word in text:
                tf_idf_val = text_collection.tf_idf(word, text)
            else:
                tf_idf_val = 0

            feat_vec.append(tf_idf_val)

        # assign value
        X[i, :] = np.array(feat_vec)
        y[i] = int(r_data['label'])

    return X, y


def get_best_model(model, X_train, y_train, params, cv=5):
    clf = GridSearchCV(model, params, cv=cv, verbose=3)
    clf.fit(X_train, y_train)
    return clf.best_estimator_
